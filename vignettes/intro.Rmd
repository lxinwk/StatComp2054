---
title: "Introduction to StatComp20054"
author: "Xin Liu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp20054}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp20054__ is a simple R package, which contains two functions. One of the _Two_Fold_Cross_Validation_ for R is to express the model in the form of linear, quadratic, exponential and double logarithm through the lack of two method, and compare the four in the form of variance the effect of model fitting. The other is the knn algorithm. Its classification idea is relatively simple. Find the K samples that are closest to it from the training samples, and then see which of the K samples has more samples, then the value to be determined (or sampling) It falls into this category. Contains three functions, respectively _knn_classi_ for R .
The two-fold cross-validation algorithm is as follows:

```{r,eval=FALSE}
function(a,b){
  n <- length(a)
  e1 <- e2 <- e3 <- e4 <- matrix(0,n,n)
  re <- numeric(4)
  for (i in 1:n) {
    
    for (j in (i+1):n-1){
      y <- a[-c(i,j)]
      x <- b[-c(i,j)]
      
      J1 <- lm(y ~ x)
      yhat1 <- J1$coef[1] + J1$coef[2] * b[c(i,j)]
      e1[i,j] <- mean((a[c(i,j)] - yhat1)^2)
      
      J2 <- lm(y ~ x + I(x^2))
      yhat2 <- J2$coef[1] + J2$coef[2] * b[c(i,j)] + J2$coef[3] *
        b[c(i,j)]^2
      e2[i,j] <- mean((a[c(i,j)] - yhat2)^2)
      
      J3 <- lm(log(y) ~ x)
      logyhat3 <- J3$coef[1] + J3$coef[2] * b[c(i,j)]
      yhat3 <- exp(logyhat3)
      e3[i,j] <-mean(( a[c(i,j)] - yhat3)^2)
      
      J4 <- lm(log(y) ~ log(x))
      logyhat4 <- J4$coef[1] + J4$coef[2] * log(b[c(i,j)])
      yhat4 <- exp(logyhat4)
      e4[i,j] <- mean((a[c(i,j)] - yhat4)^2)
      
    }
    
  }
  re <- c(2*sum(e1)/(n*(n-1)),2*sum(e2)/(n*(n-1)),2*sum(e3)/(n*(n-1)),2*sum(e4)/(n*(n-1)))
  re
}
```


E.g:
```{r,eval=FALSE}
attach(ironslag)
print(Two_Fold_Cross_Validation(chemical,magnetic))
```

The returned result is the variance obtained by fitting separately

The knn algorithm is as follows:
First calculate the distance function. :

```{r,eval=FALSE}
function(train_data,obse,type)
{
  r<-nrow(train_data)
  near_distance<-matrix(nrow=r,ncol=1)
  for(i in 1:r){
    near_distance[i]<-norm(cbind(train_data[i,]-obse),type)
  near_distance
  }
}
```

#type = c("O", "I", "F", "M", "2")
Secondly, calculate the distance relationship with all training sets:

```{r,eval=FALSE}
function(train_data,obse,type,calcols,classcol,idcol)
{
  d<-distance(train_data[,calcols],obse,type)
  m <- data.frame(train_data[,idcol],train_data[,classcol],d)
  m
}
```
Finally, use knn to predict the category the point belongs to:
```{r,eval=FALSE}
function(train_data,obse,type,calcols,classcol,idcol,k)
{
  distance<-function(train_data,obse,type)
  {
    r<-nrow(train_data)
    near_distance<-matrix(nrow=r,ncol=1)
    for(i in 1:r){
      near_distance[i]<-norm(cbind(train_data[i,]-obse),type)#zhao		
    }
    near_distance
  }
  knn<-function(train_data,obse,type,calcols,classcol,idcol)
  {
    d<-distance(train_data[,calcols],obse,type)
    m <- data.frame(train_data[,idcol],train_data[,classcol],d)
    m
  }
  all_near<-knn(train_data,obse,type,calcols,classcol,idcol)
  all_near<-all_near[order(all_near[,3]),]
  r<-nrow(all_near)
  cla<-unique(all_near[,2])
  probs<-matrix(cla,ncol=2,nrow=length(cla),byrow=FALSE)
  k_nearest<-all_near[1:k,]
  freqs<-as.data.frame(table(k_nearest[,2]))
  freqs[,2]<-freqs[,2]/k
  colnames(freqs)<-c("class","probs")
  freqs<-freqs[order(freqs[,2],decreasing = TRUE),]
  re <- freqs[1,]
  re
}
```
#train_data Training set matrix format
#obse To predict the object
#type The type of norm used to calculate the distance
#calcols Those columns are involved in calculating distance
#classcol Category Flag Class
#idcol The column that uniquely identifies each training object
#k k value in the neighborhood
E.g:
```{r,eval=FALSE}
knn_classi(iris,c(11,22),'2',c(2:3),4,1,5)
```